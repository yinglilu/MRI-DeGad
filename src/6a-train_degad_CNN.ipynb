{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    RandCropByPosNegLabeld,\n",
    "    Spacingd,\n",
    "    RandWeightedCrop,\n",
    "    RandRotate,\n",
    "    Rand3DElasticd,\n",
    "    RandRotated,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    RandFlipd)\n",
    "import tqdm\n",
    "from torchmetrics import MeanSquaredError\n",
    "import time\n",
    "from monai.networks.nets import UNet\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, Dataset ,nifti_saver, PatchDataset, DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "from glob import glob\n",
    "from monai.networks.blocks import Convolution\n",
    "from monai.networks.nets import Discriminator, Generator\n",
    "from monai.utils import progress_bar\n",
    "import torch.nn as nn\n",
    "import torchmetrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "gad_t1= sorted(glob('/home/fogunsan/scratch/degad/derivatives/passing_dataset/*/*_acq-gad_resampled_T1w.nii.gz'))# gad images who's corresponding nongad images underwent a rigid transform\n",
    "nongad_t1= sorted(glob('/home/fogunsan/scratch/degad/derivatives/normalized_fcm/*/*_acq-nongad_normalized_fcm.nii.gz')) # nongad images which underwent a rigid transform and underwent fcm normalization\n",
    "image_dict = [{\"image\": gad_name, \"label\": nongad_name} for gad_name, nongad_name in zip(gad_t1,nongad_t1)] #creates list of dictionaries, with gad and nongad images labelled\n",
    "print(len(image_dict))\n",
    "train_files, validate_files, test_files = image_dict[0:33], image_dict[33:42], image_dict[42:47] #creates a list of dictionaries for each set (training, val, testing), with keys of gad and nongad in each index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 33/33 [01:16<00:00,  2.31s/it]\n",
      "Loading dataset: 100%|██████████| 9/9 [00:27<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "num_train_files = len(train_files)\n",
    "num_validate_files = len(validate_files)\n",
    "num_patches = 20000 #patches per image\n",
    "batch_size = 10\n",
    "training_sample_size = int(num_train_files * num_patches / batch_size)\n",
    "validate_sample_size = int(num_validate_files * num_patches / batch_size)\n",
    "load_images= Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityd(keys = [\"image\"], minv=0.0, maxv=1.0)])# applying min max normalization only on gad images\n",
    "    \n",
    "train_imgs_cache = CacheDataset(data=train_files, transform=load_images)\n",
    "validate_imgs_cache = CacheDataset(data=validate_files, transform=load_images)\n",
    "\n",
    "patching_func= RandCropByPosNegLabeld( # gonna use this function to create patches\n",
    "            keys = [\"image\", \"label\"],\n",
    "            label_key = \"image\",\n",
    "            spatial_size=(32,32,32),\n",
    "            pos = 1,\n",
    "            neg = 0.0001, # much larger probability of sampling foreground\n",
    "            num_samples= num_patches# CHANGE BACK TO 5000\n",
    "        )\n",
    "patch_transforms = Compose([RandRotated(keys =[\"image\", \"label\"], range_x = [0.8,0.8], range_y = [0.8,0.8], range_z = [0.8,0.8], prob = 0.4), RandFlipd(keys =[\"image\", \"label\"], prob = 0.2, spatial_axis=1)])# flipping along y-axis (horizontally)\n",
    "\n",
    "train_patches_dataset = PatchDataset(data = train_imgs_cache, patch_func=patching_func, samples_per_image=num_patches, transform = patch_transforms)\n",
    "validate_patches_dataset = PatchDataset(data = validate_imgs_cache, patch_func=patching_func, samples_per_image=num_patches, transform = patch_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN=UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            channels=(16, 32,64,128,256,512,512,512),\n",
    "            strides=(2, 2, 2, 2,1,1,1),\n",
    "            dropout= 0.2,\n",
    "        )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "CNN.apply(monai.networks.normal_init)\n",
    "CNN_model = CNN.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading dataloader\n",
      "1/66000 epoch 1, training rmse loss: 0.0000 [                              ]\n",
      "2/66000 epoch 1, training rmse loss: 0.0000 [                              ]\n",
      "3/66000 epoch 1, training rmse loss: 0.0000 [                              ]\n",
      "4/66000 epoch 1, training rmse loss: 0.0000 [                              ]\n",
      "5/66000 epoch 1, training rmse loss: 0.0000 [                              ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m CNN_model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# setting model to training mode\u001b[39;00m\n\u001b[1;32m     16\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# total traininig loss in an epoch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,train_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# iterating through dataloader\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     progress_bar(\n\u001b[1;32m     20\u001b[0m         index\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;66;03m# displays what step we are of current epoch, our epoch number, training (rmse loss) \u001b[39;00m\n\u001b[1;32m     21\u001b[0m         count \u001b[38;5;241m=\u001b[39m training_sample_size, \n\u001b[1;32m     22\u001b[0m         desc\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, training rmse loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss_values[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m         newline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     ) \u001b[38;5;66;03m# progress bar to display current stage in training\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     cnn_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/monai/data/dataset.py:107\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Subset(dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, indices\u001b[38;5;241m=\u001b[39mindex)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/monai/data/grid_dataset.py:270\u001b[0m, in \u001b[0;36mPatchDataset._transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    268\u001b[0m image_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples_per_image)\n\u001b[1;32m    269\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[image_id]\n\u001b[0;32m--> 270\u001b[0m patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(patches) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples_per_image:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m(\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`patch_func` must return a sequence of length: samples_per_image=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples_per_image\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     )\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/monai/transforms/croppad/dictionary.py:871\u001b[0m, in \u001b[0;36mRandCropByPosNegLabeld.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    868\u001b[0m         ret[i][key] \u001b[38;5;241m=\u001b[39m deepcopy(d[key])\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d):\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, im \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcropper\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m):\n\u001b[1;32m    872\u001b[0m         ret[i][key] \u001b[38;5;241m=\u001b[39m im\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/monai/transforms/croppad/array.py:1113\u001b[0m, in \u001b[0;36mRandCropByPosNegLabel.__call__\u001b[0;34m(self, img, label, image, fg_indices, bg_indices, randomize)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, center \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters):\n\u001b[1;32m   1112\u001b[0m     roi_size \u001b[38;5;241m=\u001b[39m fall_back_tuple(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_size, default\u001b[38;5;241m=\u001b[39mlabel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m-> 1113\u001b[0m     cropped \u001b[38;5;241m=\u001b[39m \u001b[43mSpatialCrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi_center\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroi_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroi_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_track_meta():\n\u001b[1;32m   1115\u001b[0m         ret_: MetaTensor \u001b[38;5;241m=\u001b[39m cropped  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/monai/transforms/croppad/array.py:511\u001b[0m, in \u001b[0;36mSpatialCrop.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m    Apply the transform to `img`, assuming `img` is channel-first and\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    slicing doesn't apply to the channel dim.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/fogunsan.5668692.0/tmp/kslurm-venv-r7f6d20d/lib/python3.10/site-packages/monai/transforms/croppad/array.py:447\u001b[0m, in \u001b[0;36mCrop.__call__\u001b[0;34m(self, img, slices)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_track_meta():\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_meta(tensor\u001b[38;5;241m=\u001b[39mimg_t, slices\u001b[38;5;241m=\u001b[39mslices)\n\u001b[0;32m--> 447\u001b[0m     cropped_from_start \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mslices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     cropped_from_end \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(orig_size) \u001b[38;5;241m-\u001b[39m img_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m cropped_from_start\n\u001b[1;32m    449\u001b[0m     cropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chain(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(cropped_from_start\u001b[38;5;241m.\u001b[39mtolist(), cropped_from_end\u001b[38;5;241m.\u001b[39mtolist())))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "learning_rate = 2e-4\n",
    "betas = (0.5, 0.999)\n",
    "cnn_opt = torch.optim.Adam(CNN_model.parameters(), lr = learning_rate, betas=betas)\n",
    "\n",
    "start = time.time() # initializing variable to calculate training time\n",
    "max_epochs = 100 # max total iterations over entire training set\n",
    "mean_squared = MeanSquaredError(squared = False).to(device) # metric for validation calculated at the end of each epoch\n",
    "mse_error = [0] # list of validation loss calculated at the end of each epoch\n",
    "epoch_loss_values = [0] # list of training loss calculated at the end of each epoch\n",
    "train_loader = DataLoader(train_patches_dataset, batch_size=batch_size, shuffle=True)\n",
    "print('finished loading dataloader')\n",
    "val_loader = DataLoader(validate_patches_dataset, batch_size=batch_size, shuffle=True)\n",
    "for epoch in range(max_epochs):\n",
    "    CNN_model.train() # setting model to training mode\n",
    "    epoch_loss = 0 # total traininig loss in an epoch\n",
    "    for i,train_batch in enumerate(train_loader): # iterating through dataloader\n",
    "        \n",
    "        progress_bar(\n",
    "            index=i+1, # displays what step we are of current epoch, our epoch number, training (rmse loss) \n",
    "            count = training_sample_size, \n",
    "            desc= f\"epoch {epoch + 1}, training rmse loss: {epoch_loss_values[-1]:.4f}\",\n",
    "            newline = True\n",
    "        ) # progress bar to display current stage in training\n",
    "        cnn_opt.zero_grad()\n",
    "        gad_images = train_batch['image'].cuda()# gad images of batch\n",
    "        nongad_images = train_batch['label'].cuda() # nongad images of batch\n",
    "        degad_images = CNN_model(gad_images) # feeding CNN with gad images\n",
    "        mse_loss= nn.MSELoss() \n",
    "        loss = torch.sqrt(mse_loss(degad_images, nongad_images))# convert mse to rmse\n",
    "        loss.backward()\n",
    "        cnn_opt.step()\n",
    "        epoch_loss += loss.item() # adding loss for this batch to the total training loss for this epoch\n",
    "    \n",
    "    epoch_loss_values.append(epoch_loss / training_sample_size) # append total epoch loss divided by the number of steps in sample to the epoch loss list\n",
    "    CNN_model.eval() #setting model to evaluation mode for training\n",
    "    with torch.no_grad():\n",
    "        mean_squared.reset() # resetting metric state for every epoch\n",
    "        mse_total_epoch = 0 # mean squared error for the entire epoch\n",
    "        for i,val_batch in enumerate(val_loader): # iterating through dataloader\n",
    "            progress_bar(\n",
    "            index=i+1, # displays what step we are of current epoch, our epoch number,  val (rmse metric) values\n",
    "            count = training_sample_size, \n",
    "            desc= f\"epoch {epoch + 1}, validation rmse metric: {mse_error[-1]:.4f}\",\n",
    "            newline = True\n",
    "        )   #progress bar to display current stage in training\n",
    "            gad_images =val_batch[\"image\"].cuda()# batch with gad images\n",
    "            nongad_images = val_batch[\"label\"].cuda() # batch with nongad images\n",
    "            degad_images = CNN_model(gad_images)\n",
    "            val_mse = mean_squared(degad_images, nongad_images)\n",
    "            mse_total_epoch = mse_total_epoch + val_mse # adding mse of this batch to total epoch mse\n",
    "        mse_error.append(mse_total_epoch.item()/(validate_sample_size )) # dividing total mse in this epoch by the number of batches -> add to list of epoch mse\n",
    "        \n",
    "end = time.time()\n",
    "time = end - start\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('/home/fogunsan/scratch/degad/derivatives/UNET/April20/model_stats.txt', 'w') as file:  \n",
    "    file.write(f'training time: {time}\\n')  \n",
    "    file.write(f'training loss: {epoch_loss_values[-1]} validation loss: {mse_error[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CNN_model.state_dict(), \"/home/fogunsan/scratch/degad/derivatives/UNET/April20/trained_unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.semilogy(*zip(*epoch_loss_values), label=\"Training Loss\")\n",
    "plt.semilogy(*zip(*mse_error), label=\"Validation Loss\")\n",
    "plt.grid(True, \"both\", \"both\")\n",
    "plt.legend()\n",
    "plt.savefig('/home/fogunsan/scratch/degad/derivatives/UNET/April20/lossfunction.png')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
